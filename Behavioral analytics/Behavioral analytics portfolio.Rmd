---
title: "Analytics Portfolio"
author: "Fernando Torres H."
output:
  ioslides_presentation:
    theme: lumen
---



```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, fig.align="center", out.width = "650px")

```


```{r import/create data, global, include = FALSE}

# Load initial script
source("/Users/ftoresh/Google Drive/Data Analysis/Script Formulas/Initiate.R")

library(caTools)
library(caret)
# library(ggthemes)
# library(gridExtra)
# library(scales)
# library(fpc)
library(factoextra)
library(plotly)
# library(parallel)
# library(readr)
library(cluster)
library(kableExtra)
library(iml)



### CRM----

### Import data
dataset_donations <-
  read_csv(
    here::here(
      "Behavioral analytics",
      "Datasets",
      "dataset_main_donations_thesis.csv"
    )
  )

####  Prepare the Data

# 1) Create dataset
dataset_pre_clustering <-
  dataset_donations %>%
  ## filter outliers and year
  filter(!donor.no %in% c("2640"),
         donation.year > 2009) %>%
  select(donation.date,
         donation.year,
         donor.no,
         donation.amount) %>%
  # Calculate Length
  # Calculate Frequency
  group_by(donor.no) %>%
  arrange(donation.date) %>%
  ## Create a sequence to tag the first and last donation and calculate frequency
  mutate(
    count = sequence(n()),
    frequency = max(count),
    first.last.donation.tag = case_when(
      count == min(count) ~ "First Donation",
      count == max(count) ~ "Last Donation",
      TRUE ~ "Continue Donation"
    )
  ) %>%
  mutate(
    first.donation.date = case_when(first.last.donation.tag == "First Donation" ~ donation.date),
    last.donation.date = case_when(first.last.donation.tag == "Last Donation" ~ donation.date)
  ) %>%
  ### Complete all the rows of first and last donation date so it's easy to calculate the difference
  fill(first.donation.date) %>%
  ## Data has to be rearranged so that the formula fill can still fill it downwards
  arrange(desc(donation.date)) %>%
  fill(last.donation.date) %>%
  ## Rearrange again to ascendant
  arrange(donation.date) %>%
  mutate(
    length.pre = difftime(last.donation.date,
                          first.donation.date, units = "days"),
    length = mean(length.pre, na.rm = TRUE)
  ) %>%
  # Calculate Recency
  mutate(
    days.ndonation.to.last.donation = difftime(Sys.Date(),
                                               donation.date, units = "days"),
    recency = mean(days.ndonation.to.last.donation, na.rm = TRUE) 
  ) %>%
  # Calculate periodicity
  mutate(
    diff = donation.date - lag(donation.date),
    diff.days = as.numeric(diff, units = 'days'),
    peridiocity = sd(diff.days, na.rm = TRUE)
  ) %>%
  ## Calculate Monetary
  mutate(monetary = mean(donation.amount, na.rm = TRUE)) %>%
  ungroup() %>%
  ## Exclude donors that had less than two visits
  filter(frequency > 2) %>%
  ## Leave one row per donor
  distinct(donor.no, .keep_all = TRUE)


dataset_clustering <- dataset_pre_clustering %>%
  select(length,
         recency,
         frequency,
         monetary,
         peridiocity) %>%
  mutate(across(c(length,
                 recency), ~ as.numeric(.)))


# Behavioural cohorts----

# Import data 

## General survival's data
dataset_linear_survival <- 
  read_csv(
    here::here(
      "Behavioral analytics",
      "Datasets",
      "dataset_linear_survival.csv"
    )
  )

## Critical event's data
table_playlists_created <- 
  read_csv(
    here::here(
      "Behavioral analytics",
      "Datasets",
      "table_playlists_created.csv"
    )
  )

# Join survival data with critical event's data
dataset_linear_survival_playlists_created <-
  dataset_linear_survival %>%
  # Join with table that has number of playlists created
  left_join(table_playlists_created, by = "user.id") %>%
  replace_na(list(count.playlists = 0)) %>%
  mutate(
    grouped.count.playlists = case_when(
      count.playlists %in% c("0") ~ "0",
      count.playlists %in% c("1", "2", "3") ~
        "1-3",
      TRUE ~ "More"
    )
  ) %>% 
  mutate(across((grouped.count.playlists), ~ as.character(.) %>% as.factor)) %>%
  mutate(across((grouped.count.playlists), ~ factor(., levels = c("More",
                                                                  "1-3",
                                                                  "0"))))


# Engagement prediction----

## General survival's data
dataset_survived_ml <- 
  read_csv(
    here::here(
      "Behavioral analytics",
      "Datasets",
      "dataset_survived_ml.csv"
    )
  )

```

<br>

## **Lifetime Value prediction using CRM data**

### **The pipeline included two steps:**

* **Segmentation of users according to their Length, Recency, Frequency, Monetary, Periodicity of transactions**
<br>
The original publication of this model can be found by entering [this link](https://www.emeraldinsight.com/doi/abs/10.1108/MIP-11-2016-0210).

* **Fitting a model that predicts future user engagement.**
<br>
Fitting a model per cluster was found to improve the balance between Sensitivity and Specificity, as well as capture the variable importance among users with similar transactional patterns. This framework was used as a prescriptive analysis to inform the strategic goals of a non-profit organisation. 


---

```{r cluster forecast, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

# Scale the data if necessary (by preProcess = )
## Create an object with the pre processing parameters
ObjectPreprocessParams <-
  caret::preProcess(dataset_clustering, method = c("scale"))

## Transform the dataset using the parameters
dataset_clustering <-
  predict(ObjectPreprocessParams, dataset_clustering)

# Number of clusters decided to use
NumberClusters <- 5

set.seed(29)
ObjectKM <- kmeans(dataset_clustering,
                   NumberClusters,
                   iter.max = 300,
                   nstart = 10)

## Create vector with cluster assignments
VectorClusters <- ObjectKM$cluster
## Create a new column with the clients and the cluster in which it was assigned
## If dataset_pre_clustering was created, use it here
dataset_clustered_km <-
  mutate(dataset_pre_clustering, cluster.assigned = VectorClusters)

factoextra::fviz_cluster(
  ObjectKM,
  dataset_clustering,
  palette = c("#6A453B", "#CEAA7A", "#E9E2CE", "#7399BD", "#1C477F"),
  # palette = c("#da6366", "#0099CC", "#99E6FF", "#6cc473", "#6e6e9e"),
  alpha = 0.7,
  ggtheme = theme_minimal(),
  main = "Partitioning Clustering Plot",
  labelsize = 0
) +
  labs(title = "Clusters of LRFMP Variables in a Two-Dimensional Space",
       subtitle = "Principal Components were used to reduce dimensions")

```

Five segments were identified, each with their own transactional patterns. A characterization was performed after further analysis of the clusters. 

---

```{r ML model stats, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

ObjectTableML <- 
  read_csv("~/Google Drive/Portfolio/Table2 (csv).csv")

kable(ObjectTableML) %>% 
  kable_styling(bootstrap_options = "condensed", position = "center")

```


A classification model was performed, gathering concepts from Churn Prediction and Lifetime Value models, adapting them to the problem to solve. 



## **Cohort analysis**


Be able to directly see which behaviour drove to better or worse retention is key in building features that improve Lifetime Value.


-----

```{r Survival analysis for churn, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

# Survival with labelling
classifier_survival_playlists_created <-
  survival::survfit(survival::Surv(time, type.user) ~
                      grouped.count.playlists,
                    data = dataset_linear_survival_playlists_created)


PlotSurvivalCI(classifier_survival_playlists_created,
             "More than 3 times",
             "1-3",
             "0",
             legendtitle = "Event\n performed") +
  geom_hline(
    yintercept = 0.44,
    lty = "longdash",
    alpha = 0.5,
    color = "#A09D98"
  ) +
  annotate(
    "text",
    x = 15,
    y = 0.4,
    label = "Average retention",
    size = 4,
    alpha = 0.5
  ) +
  coord_cartesian(ylim = c(0.3, 1)) +
  coord_cartesian(xlim = c(0, 91)) +
  labs(title = "90 days retention by Behavioural Cohort",
       x = "Days since subscription")

```



## **Understand Machine Learning predictions**

<br>
By predicting a particular behaviour, such as churning or any other user interaction, the model will decode ,from vast amounts of data, what drove users to perform that activity. However, Machine Learning models are black-boxes, so predicting is useless if there is no clear understanding of the models' learnings.

Explainable Machine Learning techniques are used to make models become human readable. Performing this analysis can save hours of data exploration, as the model is able to get the patterns more efficiently. 

----

```{r Explainable ML (modelling), fig.align="center", include = FALSE, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

### Create ML model that predicts engagement

# Randomize dataset
random <- sample(nrow(dataset_survived_ml))
dataset_survived_ml <- dataset_survived_ml[random,]

# Training and test set
set.seed(123)
split <-
  caTools::sample.split(dataset_survived_ml$binari.engaged, SplitRatio = 0.7)
training_set <- subset(dataset_survived_ml, split == TRUE)
test_set <- subset(dataset_survived_ml, split == FALSE)

# Create custom indices: myFolds
myFolds <- createFolds(training_set$binari.engaged, k = 5)

my_control <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = TRUE,
  savePredictions = FALSE,
  index = myFolds
)

# Parallel computing
# Calculate the number of cores
no_cores <- parallel::detectCores() - 1

# Initiate cluster
cl <- parallel::makeCluster(no_cores, type = "FORK")

# Binary Output
classifier <-
  train(
    binari.engaged ~ .,
    data = training_set,
    trControl = my_control,
    # tuneGrid = tune_grid,
    method = "xgbTree"
  )

# Stop parallel computing
parallel::stopCluster(cl)


```


```{r Explainable ML (explanation), fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

## Make Object predictor
dataset_explainable_ml <- training_set[which(names(training_set) != "binari.engaged")] 
  
## Create a predictor object with training data with and without dependent variable
ObjectPredictor <- Predictor$new(classifier, data = dataset_explainable_ml, y = training_set$binari.engaged)


### Feature Effect (FE)----

ObjectFE <-
  FeatureEffect$new(
    ObjectPredictor,
    # Feature to asses the effect on prediction
    feature = "online.playlist.perc.listened",
    # Method to use
    method = "ale",
    # X axis
    grid.size = 30
  ) 


ObjectFE$results %>%
  filter(.class == "Yes") %>%
  mutate(online.playlist.perc.listened.mod = online.playlist.perc.listened / 100) %>%
  ggplot(aes(online.playlist.perc.listened.mod, .value)) +
  geom_line(color = "#CEAA7A",
            size = 1,
            alpha = 0.8) +
  geom_line(color = "#CEAA7A",
            size = 3,
            alpha = 0.2) +
  my_minimal_theme +
  geom_vline(xintercept = 0.9,
             colour = "deepskyblue3",
             alpha = 0.3) +
  geom_vline(xintercept = 1,
             colour = "deepskyblue3",
             alpha = 0.3) +
  annotate(
    "rect",
    xmin = 0.9,
    xmax = 1,
    ymin = -Inf,
    ymax = Inf,
    fill = "deepskyblue3",
    alpha = .2,
    color = NA
  ) +
  annotate(
    "text",
    x = 0.7,
    y = 0.05,
    label = "Disengagement\n area",
    size = 4,
    alpha = 0.7
  ) +
  geom_segment(
    aes(
      x = 0.82,
      y = 0.05,
      xend = 0.95,
      yend = 0.05
    ),
    color = "#CEAA7A",
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  labs(title = "How does the usage of a feature affect engagement?",
       x = "Usage of a feature",
       y = "Change in Engagement") +
  scale_x_continuous(
    limits = c(0, 1),
    breaks = seq(0, 1, by = 0.10),
    labels = scales::percent_format(accuracy = 5L)
  ) 


```


## **Interactive Web-Dashboards using reactive programming**

* **User behaviour tracker.**
<br>
User activity, critical events or activation points need monitoring. Being able to constantly see the changes in user behaviour and determine if goals are met can be game changer. To see an example access [this link](https://ftoresh.shinyapps.io/portfolio_dash/)


* **Rapid prototyping.**
<br>
Prototyping dashboards to get approval from stakeholders. Once approved, they can be replicated into a more complex information system. 


## **Merge data from different sources to form a unique reality**

Click [here](https://www.youtube.com/watch?v=22a7iOIU6sw&feature=youtu.be) to see a real example of data gathered from two different sources, then joined to form a unique reality on a geospacial dashboard. 


---

The code of this portfolio can be seen on my Github account following [this repository](https://github.com/ftoresh/Projects-Portfolio/blob/master/Portfolio%20presentation%20script.Rmd). 
