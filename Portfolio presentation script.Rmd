---
title: "Data Science Portfolio"
author: "Fernando Torres H."
output:
  ioslides_presentation:
    theme: lumen
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center", out.width = "650px")

```


```{r import/create data, global, include = FALSE}

library(readxl)
library(caTools)
library(forecast)
library(caret)
library(tidyverse)
library(astsa)
library(ggthemes)
library(gridExtra)
library(scales)
library(fpc)
library(factoextra)
library(plotly)
library(parallel)
library(readr)
library(cluster)
library(kableExtra)


### Revenue Management----

### Parameters
value_hotel<- "Cucuve"
limite_occ<-  70

### Main revenue dataset
# 1) Import multiple files (Base)

## Set working directory (Files before February 2018)
setwd("~/Google Drive/Data Analysis/Bases de Datos/Colibri/Data Hoteles")

## Create a list from these files
list.filenames<-list.files(pattern=".xls$")
# list.filenames

## Create an empty list that will serve as a container to receive the incoming files
list.data<-list()

## Create a loop to read in your data
for (i in 1:length(list.filenames))
{
  list.data[[i]]<-read_excel(list.filenames[i], skip = 0)
}

## Add the names of your data to the list
names(list.data)<-list.filenames

## Join all the datasets of the list into one dataset
dataset<- bind_rows(list.data, .id = "Month")

# Pick the relevant information
## Delete empty columns 
dataset<- Filter(function(x)!all(is.na(x)), dataset)
dataset <- dataset[,colSums(is.na(dataset))<nrow(dataset)] 

# Delete empty NA rows
dataset<- dataset %>% na.omit()

##Change names to dataset
dataset<-  dataset %>% select(Month, Date, Available_Rooms = 'Available Rooms', Night_Sold = 'Night Sold', Complimentary, Occ = 'Occ%' ,ADR, Rev_Par = 'Rev Par', Pax, Room_charges = 'Room Charges') 

## Remove unwanted "," from numbers
dataset<- dataset %>% mutate_if(is.character, funs(gsub(",","",.)))

## Change to numeric the columns
dataset<- mutate_at(dataset, vars(Available_Rooms:Room_charges), funs(as.numeric))

## Separate column Date into day in words and number
dataset<- separate(dataset, Date, c("Date", "Day"))

## Leave the number of the month and year without .xls
dataset<- separate(dataset, Month, c("Hotel", "Month", "Year"))

## Create new variables
## Create variable Wholeyear_week  
dataset<- mutate(dataset, Complete_date = paste(dataset$Year,"/", dataset$Month,"/", dataset$Date, sep = "" ))

dataset$Complete_date<- as.Date(dataset$Complete_date, format = "%Y/%m/%d")
Wholeyear_week<- strftime(dataset$Complete_date, format = "%V")
dataset$Wholeyear_week <- Wholeyear_week

## Create variable Night_Sold_Complete
dataset<- dataset %>% mutate(Night_Sold_Complete = Night_Sold + Complimentary)

## Create variable Holiday
holidays<- as.Date(c("01-January-2016", "08-February-2016", "09-February-2016", 
                     "25-March-2016", "24-May-2016", "01-May-2016", "10-August-2016", 
                     "09-October-2016","02-November-2016", "03-November-2016", 
                     "25-December-2016", "01-January-2017", "02-January-2017", "27-February-2017", 
                     "28-February-2017", "14-April-2017", "01-May-2017", "24-May-2017", 
                     "10-August-2017", "09-October-2017", "02-November-2017", "03-November-2017", 
                     "25-December-2017"), format = "%d-%B-%Y") %>%
  strftime(format = "%V")

dataset<- mutate(dataset, Holiday = ifelse(dataset$Wholeyear_week %in% holidays, "yes", "no"))

## Create variable GoodWeather (june to november cold, november june warm)
dataset<- mutate(dataset, GoodWeather = ifelse(dataset$Month %in% c("01","02","03","04","05","06","11","12"), "yes", "no"))

## Create variable Weekend
dataset<- mutate(dataset, Weekend = ifelse(dataset$Day %in% c("Sat", "Sun"), "yes", "no"))

## Create variable Ciudad
dataset<- dataset %>% mutate(Ciudad = case_when(Hotel %in% c("Cucuve", 
                                                             "Angermeyer", 
                                                             "Aquamarine", 
                                                             "Cottages", 
                                                             "Espana",
                                                             "GalapagosSuites",
                                                             "LaZayapa",
                                                             "SantaFe" ) ~ "Galapagos",
                                                Hotel %in% "LaTerraza" ~ "Puerto Lopez"))


## Create variable ADR_Mercado
dataset_ADR_Galapagos<- filter(dataset, Ciudad == "Galapagos") %>%
  group_by(Ciudad, Complete_date) %>%
  summarise(ADR_Mercado = mean(ADR, na.rm = TRUE)) %>%
  ungroup() %>% 
  gather(Ciudad_ADR, ADR_Mercado, -c(Ciudad, Complete_date)) %>%
  select(Ciudad, Complete_date, ADR_Mercado)

dataset_ADR_PuertoLopez<- filter(dataset, Ciudad == "Puerto Lopez") %>%
  group_by(Ciudad, Complete_date) %>%
  summarise(ADR_Mercado = mean(ADR, na.rm = TRUE)) %>%
  ungroup() %>% 
  gather(Ciudad_ADR, ADR_Mercado, -c(Ciudad, Complete_date)) %>%
  select(Ciudad, Complete_date, ADR_Mercado) 

dataset<- union(dataset_ADR_Galapagos, dataset_ADR_PuertoLopez) %>% left_join(dataset, by = c("Complete_date", "Ciudad"))

# Create variable binari when Occ > x%
dataset<- dataset %>% mutate(OccBinari = case_when(Occ >= limite_occ ~ "Yes",
                                                   Occ <= limite_occ ~ "No" ))

## Order the dataset by date
dataset<- dataset %>% arrange(Complete_date)

# Convert to factor 
dataset$Ciudad <- as.factor(dataset$Ciudad)
dataset$Hotel <- as.factor(dataset$Hotel)


### Create National_Index dataset


# 1) Import the data 
## Import National Index
## Import complete dataset of national indexes to make forecast 
setwd("~/Google Drive/Data Analysis/Bases de Datos/Colibri/Data complementaria")
# Be aware that there are empty spaces in the two next months of the dataset
dataset_national_index_complete<- read_excel("~/Google Drive/Data Analysis/Bases de Datos/Colibri/Data complementaria/dataset_national_index_excel.xlsx", sheet = "Sheet 1", skip = 1)


### CRM----

### Import data
dataset_donations <- read_csv("~/Google Drive/Data Analysis/Bases de Datos/St. Cuthberts/Support Services/Donorflex exports/dataset_main_donations_thesis.csv")

####  Prepare the Data 

# 1) Create dataset
dataset_pre_clustering <-
  dataset_donations %>%
  # group_by(donation.year, donor.no) %>%
  # mutate(
  #   number.donation.year = n(),
  #   value.donations.year = mean(donation.amount, na.rm = TRUE),
  #   mean.number.donations = mean(number.donation.year, na.rm = TRUE),
  #   mean.value.donations = mean(value.donations.year, na.rm = TRUE)
  # ) %>%
  # ungroup() %>%
  filter(
    ## This donors seems to be outliers
    !donor.no %in% c("2640"), 
    donation.year > 2009) %>%
  select(donation.date,
         donation.year,
         donor.no,
         donation.amount) %>%
  # Calculate Length
  # Calculate Frequency
  group_by(donor.no) %>%
  arrange(donation.date) %>%
  ## Create a sequence to tag the first and last donation and calculate frequency
  mutate(
    count = sequence(n()),
    frequency = max(count),
    first.last.donation.tag = case_when(
      count == min(count) ~ "First Donation",
      count == max(count) ~ "Last Donation",
      TRUE ~ "Continue Donation"
    )
  ) %>%
  mutate(
    first.donation.date = case_when(first.last.donation.tag == "First Donation" ~ donation.date),
    last.donation.date = case_when(first.last.donation.tag == "Last Donation" ~ donation.date)
  ) %>%
  ### Complete all the rows of first and last donation date so it's easy to calculate the difference
  fill(first.donation.date) %>%
  ## Data has to be rearranged so that the formula fill can still fill it downwards
  arrange(desc(donation.date)) %>%
  fill(last.donation.date) %>%
  ## Rearrange again to ascendant
  arrange(donation.date) %>%
  mutate(
    length.pre = difftime(last.donation.date,
                          first.donation.date, units = "days"),
    length = mean(length.pre, na.rm = TRUE)
  ) %>%
  # Calculate Recency
  mutate(
    days.ndonation.to.last.donation = difftime(Sys.Date(),
                                               donation.date, units = "days"),
    recency = mean(days.ndonation.to.last.donation, na.rm = TRUE) ## pendant to limit number of recent donations
  ) %>%
  mutate(
    diff = donation.date - lag(donation.date),
    diff.days = as.numeric(diff, units = 'days'),
    peridiocity = sd(diff.days, na.rm = TRUE)
    ## Average number of days between donations
    # days.between.donations = mean.diff.days / frequency
  ) %>%
  ## Calculate Monetary
  mutate(monetary = mean(donation.amount, na.rm = TRUE)) %>%
  ungroup() %>%
  ## Exclude donors that had less than two visits
  filter(frequency > 2) %>% 
  ## Leave one row per donor
  distinct(donor.no, .keep_all = TRUE) 


dataset_clustering <- dataset_pre_clustering %>%
  select(length,
         recency,
         frequency,
         monetary,
         peridiocity) %>% 
  mutate_at(vars(length, 
                 recency), funs(as.numeric))

```


<br>

## **Price Optimisation ML Model**

### **The pipeline included three steps:**

* **Forecast of the average price of luxury hotels in the region.**
<br>
In the feature creation phase this variable was found to be a good predictor.

* **Find the optimal price that yields the highest probability of having a high room occupancy.**
<br>
A classification model using the Random Forest algorithm was used. This is a novel approach to price optimization, as Operational Research practices would normally use other methods, however more difficult to put into production.

* **Reporting**
<br>
Easy to read visualizations that help Revenue Managers make decisions about the pricing.

<br>


```{r fit forecast, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

#### Forecast ADR_Lux
# Make dataset a time series
vectorT<- dataset_national_index_complete %>% select(National_ADR_Lux) %>% as.ts()

# Main Information
value_week<- strftime(as.Date(Sys.Date(), format = "%Y/%m/%d"), format = "%V")
forecast_window<- length(which(is.na(dataset_national_index_complete$National_ADR_Lux))) # Months to forecast ADR_Lux
test_window<- round(length(vectorT)*0.2)

### Forecast National ADR_Lux

## Chose the best model that fits the data
## Make the test set
train <- subset(vectorT, end = length(vectorT) - test_window) 

## Fit the desired model to the training data 
lambda<- vectorT %>% BoxCox.lambda()
classifierT1 <- auto.arima(train, lambda = lambda, stepwise = FALSE, seasonal = FALSE)

# Fit ARIMA model tunning the parameters

# Trial of different values p,q,d related to the ones of auto.arima 
## Check which values of p,q,d make the AIC & BIC smaller
# diff(vectorT) %>% sarima(0,1,2) 

# ## Fit the the ARIMA model with the new paramethers
# lambda<- vectorT %>% BoxCox.lambda()
# classifierT11 <- train %>% Arima(order = c(0,1,2), lambda = lambda, seasonal = FALSE)
# 
# ## Check that model has white noise residuals
# checkresiduals(classifierT11)
# 
# ## Use accuracy() to compare the paramethers of auto.arima vs the manual fit
# for_classifierT1 <- forecast(classifierT1, h = forecast_window) 
# for_classifierT11 <- forecast(classifierT11, h = forecast_window)

# Produce forecasts for each model
## Train full time series
classifierFull1 <- auto.arima(vectorT, lambda = lambda, stepwise = FALSE, seasonal = FALSE)
# classifierFull11 <- vectorT %>% Arima(order = c(0,1,2), lambda = lambda, seasonal = FALSE)

## Forecast full time series
for_classifierFull1 <- forecast(classifierFull1, h = forecast_window)
# for_classifierFull11 <- forecast(classifierFull11, h = forecast_window)
National_ADR_Lux_predicted<- mean(for_classifierFull1$mean)



# 2) Join national index with dataset

# Replace unknown values with forecasted values
dataset_national_index_complete<- dataset_national_index_complete %>% replace_na(list(National_ADR_Lux = National_ADR_Lux_predicted))

## Convertir en factor el mes
dataset_national_index_complete$Month<- as.factor(dataset_national_index_complete$Month)
dataset_national_index_complete$Year<- as.factor(dataset_national_index_complete$Year)

# Joint national index with dataset by month and year
dataset<- left_join(dataset, dataset_national_index_complete, by = c("Month", "Year"))

## Factoring variables
dataset<- mutate_at(dataset, vars(c(Hotel:Day), Wholeyear_week, Holiday:Weekend), funs(as.factor))

# Convert NA values
dataset$ADR <- ifelse(is.na(dataset$ADR), 
                      ave(dataset$ADR, FUN = function(x) mean(x, na.rm = TRUE)), dataset$ADR)

# Eliminate ADR = 0
temp<- dataset %>% filter(Hotel == value_hotel)
dataset$ADR[dataset$ADR == 0] <- mean(temp$ADR, na.rm = TRUE)
remove(temp)
```

<br>


---

```{r plot forecast, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

############################################# Plot Forecast ADR_Lux ###########################################################

## Plot the Forecast

autoplot(for_classifierFull1, PI = TRUE) + 
  autolayer(vectorT, series = "Real Values", colour = TRUE) + 
  autolayer(for_classifierFull1$fitted, series = "Fitted Values", colour = TRUE) +
  scale_color_manual(labels = c("Fitted Values", "Real Values"),
                    values=c("deepskyblue3", "darkorange1")) +
  labs(title = "ADR Forecast", x = "Month", y = "ADR") +
  # theme_economist() + 
  # scale_color_economist(labels=c("")) + 
  theme(plot.title = element_text(size=13), 
        plot.subtitle = element_text(size = 12), 
        text = element_text(family = "Tahoma"), 
        legend.position = "right") +
  guides(color=guide_legend(title = "")) + theme_void()

```

Average price of luxury hotels in the region forecasting using ARIMA (0,1,1); this value is later used as a predictor of the main model. As shown, the values predicted by the model clearly follow the real values' trend. 

---

```{r train occ.binari, include = FALSE, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

### Train

# Select the columns that are relevant
datasetBB<- dataset %>% select(Ciudad, 
                               ADR_Mercado, 
                               Hotel, 
                               Month, 
                               Date, 
                               Wholeyear_week, 
                               Holiday, 
                               Weekend, 
                               GoodWeather, 
                               National_ADR_Lux, 
                               OccBinari,
                               ADR)


# Randomize dataset
random<- sample(nrow(datasetBB))
datasetBB<- datasetBB[random, ]

# Separar el training del test set
# library(caTools)
set.seed(123)
## El sample.split se lo efectua con la variable dependiente
split<- sample.split(datasetBB$OccBinari, SplitRatio = 0.7)

# Crear los sets de training y test
training_set<- subset(datasetBB, split == TRUE)
test_set<- subset(datasetBB, split == FALSE)

# 4) Train the model
# library(caret)

# Create custom indices: myFolds
## This is for being able to run different models with the same folds and bein able to compare them (apples with apples)
myFolds <- createFolds(training_set$OccBinari, k = 5)

############# BINARY OUTPUT

# If we want to have the ROC index of a binary classification algoritm instead of Accuracy
my_control <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = TRUE,
  savePredictions = TRUE,
  index = myFolds
)


# Calculate the number of cores
library(parallel)
no_cores <- detectCores() - 1

# Initiate cluster
cl <- makeCluster(no_cores, type="FORK")


################ BINARY OUTPUT
classifier <- train(OccBinari ~.,
                    data = training_set,
                    metric ="ROC", 
                    method = "rf",
                    tuneLenght = 10, 
                    ntree = 500,
                    trControl = my_control
)


# Stop parallel computing
stopCluster(cl)

# Print or plot the model to see the best hyperparamethers (optional)
## General information
# print(classifier)
## The graph will show the tunning paramethers in X axis, and the indicator of its accuracy 
## The best tunning paramether will be the one with the highest point on Y axis
# plot(classifier)
## Print a summary of the final model
# classifier$finalModel
# classifier$resample
## After running the model see what variables were the most important
# varImp(object = classifier) 


# 5) Model accuracy estimation

# Make a confusion matrix
y_pred <- predict.train(classifier, test_set, type = "raw")


# Select the week
week<- dataset %>% filter(Wholeyear_week == value_week, Hotel == value_hotel) 

Ciudad<- week$Ciudad
ADR_Mercado<- rep(240, length(week$Month)) 
Hotel<- value_hotel
Month<- week$Month
Date<- week$Date
Wholeyear_week<- week$Wholeyear_week
Holiday<- week$Holiday
Weekend<- week$Weekend
GoodWeather<- week$GoodWeather
National_ADR_Lux<- rep(National_ADR_Lux_predicted, length(week$Month))

```



```{r predict occ.binari, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, out.width = "700px"}
############################################# Predict Occ.Binari ################################## 

### Table with top 5 ADRs



# # Select the price that the hotel wants to set
# ADRProp<- 100

## Get the historic minimum and maximum price 
table_adr_min_max_hotel<- dataset %>% 
  filter(Hotel == value_hotel) %>% 
  summarise(max = round(max(.$ADR)), min = round(min(.$ADR))) 

## Create a vector that has from the lowest to the highest price values
## Every value is repeated N times, bein n the lenght of the hotel's predict table of the week 
Vector_ADR_Min_Max_Hotel<- rep(seq(table_adr_min_max_hotel$min, table_adr_min_max_hotel$max), each = length(Ciudad))

## Make a dataset with ids, from 1 to length hotel's predict table, from 1 to length Vector_ADR_Min_Max_Hotel
adr_min_max_hotel_dataset<- data.frame(seq1.14 = rep(1:length(Ciudad), length.out = length(Vector_ADR_Min_Max_Hotel)), adr.min.max.hotel = Vector_ADR_Min_Max_Hotel) %>%
  mutate(seq1.n = seq(1, (nrow(.))))


# Ciudad<- week$Ciudad
# ADR_Mercado<- rep(240, length(week$Month)) 
# Hotel<- value_hotel
# Month<- week$Month
# Date<- week$Date
# Wholeyear_week<- week$Wholeyear_week
# Holiday<- week$Holiday
# Weekend<- week$Weekend
# GoodWeather<- week$GoodWeather
# National_ADR_Lux<- rep(National_ADR_Lux_predicted, length(week$Month))

# Make dataframe that joins hotel's predict table of the week and dataset with min to max ADR values
predict_dataframe1.n<- data.frame(Ciudad, 
                                  ADR_Mercado, 
                                  Hotel, 
                                  Month, 
                                  Date, 
                                  Wholeyear_week, 
                                  Holiday, 
                                  Weekend, 
                                  GoodWeather, 
                                  National_ADR_Lux) %>%
  mutate(seq1.14 = seq(1, (nrow(.)), by = 1)) %>%
  full_join(adr_min_max_hotel_dataset, by = "seq1.14") %>%
  select(Ciudad, 
         ADR_Mercado, 
         Hotel, 
         Month, 
         Date, 
         Wholeyear_week, 
         Holiday, 
         Weekend, 
         GoodWeather, 
         National_ADR_Lux,
         ADR = adr.min.max.hotel,
         seq1.n) %>%
  arrange(ADR) %>%
  left_join(adr_min_max_hotel_dataset, by = "seq1.n") %>%
  select(-adr.min.max.hotel) 

## Create table to predict using ML model
predict_dataframe<- predict_dataframe1.n %>%
  select(-seq1.n)

## Make table with top 5 ADR according to prob occ > n
y_pred2 <- predict.train(classifier, predict_dataframe, type = "prob") %>%
  mutate(seq1.n = seq(1, (nrow(.)))) %>%
  left_join(predict_dataframe1.n, by = "seq1.n") %>%
  # mutate_at(vars(ADR), funs(as.factor)) %>%
  mutate_at(vars(No:Yes), funs(round(., 2))) %>%
  group_by(ADR) %>%
  summarise(mean.prob.no = mean(No), mean.prob.yes = mean(Yes)) %>%
  arrange(desc(mean.prob.yes)) %>%
  head(n =5)

y_pred2_max.profit<- predict.train(classifier, predict_dataframe, type = "prob") %>%
  mutate(seq1.n = seq(1, (nrow(.)))) %>%
  left_join(predict_dataframe1.n, by = "seq1.n") %>%
  # mutate_at(vars(ADR), funs(as.factor)) %>%
  mutate_at(vars(No:Yes), funs(round(., 2))) %>%
  group_by(ADR) %>%
  summarise(mean.prob.no = mean(No), mean.prob.yes = mean(Yes)) %>%
  arrange(desc(ADR, mean.prob.yes)) %>%
  head(n =5)

# View(y_pred2)
# View(y_pred2_max.profit)
### Graph with top 1 ADR

ADR_Opciones<- y_pred2$ADR 
ADR<- rep(y_pred2$ADR[1], length(week$Month))
ADRProp<- mean(ADR)

# Data frame with information of the day
predict_dataframe<- data.frame(Ciudad, ADR_Mercado, Hotel, Month, Date, Wholeyear_week, Holiday, Weekend, GoodWeather, National_ADR_Lux, ADR)

y_pred3 <- predict.train(classifier, predict_dataframe, type = "prob")

Prob.under<- y_pred3$No
Prob.over<- y_pred3$Yes

# Graficar porcentaje probabilidad de tener ocupación superior al x%
data.frame(AboveDesired = round(mean(Prob.over),2), BelowDesired = round(mean(Prob.under), 2)) %>%
  gather(Index, Value) %>%
  ggplot() +
  geom_col(aes(Index, Value), fill = "deepskyblue3", alpha = 0.7) + 
  annotate("text", x = 1, y = round(mean(Prob.over),2)/2, label = round(mean(Prob.over),3)*100, colour = "white") +
  annotate("text", x = 2, y = round(mean(Prob.under),2)/2, label = round(mean(Prob.under),3)*100, colour = "white") +
  theme_minimal() +
  labs(title = paste("Hotel Test"), subtitle = paste("Probability of Occupation Above", limite_occ, "%", "According to Price ($):", ADR_Opciones[1], "|", ADR_Opciones[2]), x = "", y = "Probability") + 
  # theme_economist() + 
  # scale_color_economist() +
  theme(plot.title = element_text(size=20), 
        plot.subtitle = element_text(size = 12), 
        text = element_text(family = "Tahoma"), 
        legend.position = "right") +
   scale_y_continuous(limits = c(0, 1),
                     labels = percent) 

```

In this example, on the date of analysis there is a `r round(mean(Prob.under),3)*100`% probability of having an occupancy rate below the threshold defined with the customer (70%). This low occupancy is expected even with the optimal price determined by the model.


<br>

## **Lifetime Value prediction using CRM data**

### **The pipeline included two steps:**

* **Segmentation of users according to their Length, Recency, Frequency, Monetary, Periodicity of transactions**
<br>
The original publication of this model can be found by entering [this link](https://www.emeraldinsight.com/doi/abs/10.1108/MIP-11-2016-0210).

* **Fitting a model that predicts future user engagement.**
<br>
Fitting a model per cluster was found to improve the balance between Sensitivity and Specificity, as well as capture the variable importance among users with similar transactional patterns. This framework was used as a prescriptive analysis to inform the strategic goals of a non-profit organisation. 


---

```{r cluster forecast, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

# Scale the data if neccesary (by preProcess = )
## Create an object with the pre processing parameters
ObjectPreprocessParams <- preProcess(dataset_clustering, method=c("scale"))
# summarize transform parameters
# print(ObjectPreprocessParams)
## Transform the dataset using the parameters
dataset_clustering <- predict(ObjectPreprocessParams, dataset_clustering)

# Number of clusters decided to use
NumberClusters<- 5

set.seed(29)
ObjectKM<- kmeans(dataset_clustering, 
                  NumberClusters, 
                  iter.max = 300, 
                  nstart = 10) 

## Create vector with cluster assignements 
VectorClusters<- ObjectKM$cluster
## Create a new column with the clients and the cluster in which it was assigned
## If dataset_pre_clustering was created, use it here
dataset_clustered_km <- mutate(dataset_pre_clustering, cluster.assigned = VectorClusters) 

fviz_cluster(ObjectKM, 
             dataset_clustering,
             palette = c("#da6366","#0099CC", "#99E6FF", "#6cc473", "#6e6e9e"),
             alpha = 0.5,
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot"
             ) +
  labs(title = "Clusters of LRFMP Variables in a Two-Dimensional Space",
       subtitle = "Principal Components were used to reduce dimensions")

```

Five segments were identified, each with their own transactional patterns. A characterization was performed after further analysis of the clusters. 

---

```{r ML model stats, fig.align="center", echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}

ObjectTableML <- read_csv("~/Google Drive/Portfolio/Table2 (csv).csv")

kable(ObjectTableML) %>% kable_styling(bootstrap_options = "condensed", position = "center")

```


A classification model was performed, gathering concepts from Churn Prediction and Lifetime Value models, adapting them to the problem to solve. 


<br>

## **Interactive Web-Dashboards using reactive programming**

* **Operational Dashboards.**
<br>
As operational tools or managerial information systems, dashboards can inform the business about KPI's performance. To see an example access [this link](https://ftoresh.shinyapps.io/portfolio_dash/)


* **Rapid prototyping.**
<br>
Prototyping dashboards to get approval from stakeholders. Once approved, they can be replicated into a more complex information system. 


## **Merge data from different sources to form a unique reality**

Clic [here](https://www.youtube.com/watch?v=22a7iOIU6sw&feature=youtu.be) to see a real example of data gathered from two different sources, then joined to form a unique reality on a geospacial dashboard. 


---

The code of this portfolio can be seen on my Github account following [this repository](https://github.com/ftoresh/Projects-Portfolio/blob/master/Portfolio%20presentation%20script.Rmd). 
